---
title: "624: project2 - JZ"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{r message=FALSE}
library(dplyr)
library(psych)
library(purrr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(caret)
library(mice)
library(stringr)
library(earth)
library(rpart)
library(Cubist)
library(gbm)
library(randomForest)
library(nnet)
```
## Data Understanding

  There are total 2571 observations and 33 variables. Among these variables, only `brand.code` is categorical, the rest are numeric including our target `ph`. In addtion, most of variables have more or less missing values which we need to deal with later.
  
```{r}
# read file
raw.data <- readxl::read_excel("StudentData - TO MODEL.xlsx") %>% as.data.frame()
test.data <- readxl::read_excel("StudentEvaluation- TO PREDICT.xlsx") %>% as.data.frame()
raw.data['Brand Code'] <- raw.data$`Brand Code` %>% as.factor()
test.data['Brand Code'] <- test.data$`Brand Code` %>% as.factor()
names(raw.data) <- names(raw.data) %>% str_replace_all(" ", ".") %>% tolower()
names(test.data) <- names(test.data) %>% str_replace_all(" ", ".") %>% tolower()
str(raw.data)
summary(raw.data)
```

## Visualizations

#### Categorical Distribution

  From the plot of `distribution of Brand Code` and `distribution of Brand Code by PH`, we found,
  
    1. category B have more observations than others regardless of missing value. 
    2. all categories have similar value range from roughly 8.0 to 9.0 regardless of missing value. 
  
  Therefore, 
  
    which means that there is no significant ordinary data fault that we can discard the missing value from variable `Brand Code`, we are going to impute the missing value.
```{r warning=FALSE}
# distributions of brand code
raw.data %>% 
  ggplot(aes(x = brand.code)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("distribution of Brand code")
```
```{r warning=FALSE}
# categorical dist by PH
raw.data %>% 
  ggplot(aes(x = ph, color = brand.code)) +
  geom_density() +
  theme_minimal() +
  ggtitle("distribution of Brand Code by PH")
```

#### Numeric Distribution

  From the "distribution of numeric variable against PH", we found
  
    1. some variables are skewed
    2. predictors `Pressure Vacuum` and `Mnf Flow` contains negative values
    3. variables are not in the same scale
    
  Therefore, 
    
    1. we need to transform skewed variables
    2. use transformation method which handle both positive and negative value
    3. scaling is recommanded
    
```{r warning=FALSE, message=FALSE}
# extract numeric variables
numeric.variables <- 
  raw.data %>% 
  select(-c(brand.code)) %>% 
  gather("key", "value")

# numeric dist by PH
numeric.variables%>% 
  ggplot(aes(x = value)) +
  geom_histogram() + 
  theme_minimal() +
  facet_wrap(~key, scales = "free") +
  ggtitle("distribution of nummeric variables against PH")
```
#### Outliers

  From "Box Plot of All Numeric Variables", we found,
  
    1. `Alch Rel` has 1 extreme outlier
    2. `PH` has 1 extreme outlier
    3. `Oxygen Filler` has 1 extreme outlier
    
  After examine these three outliers, we also find out that they do not present in the same case. Consider `PH` is our target variable, there is no need for further imputation, we are going to remove case generated by extreme PH value. And impute appropriate values for each of predictors.
  

```{r}
numeric.variables %>% 
  ggplot(aes(x = value)) +
  geom_boxplot() + 
  theme_minimal() +
  facet_wrap(~key, scales = "free") +
  ggtitle("Boxplot for All Numeric Variables")
```
```{r}
# extreme outlier case
which.max(raw.data$ph)
which.max(raw.data$oxygen.filler)
which.min(raw.data$alch.rel)
```
```{r}
raw.data <- raw.data[-which.max(raw.data$ph), ]
raw.data <- raw.data %>% filter(!is.na(ph))
raw.data[which.max(raw.data$oxygen.filler), ] <- NA
raw.data[which.min(raw.data$alch.rel), ] <- NA
```

#### Correlations

  From the correlation plot, we found that 
  
    1. there are some pair of variables highly correlated with correlated score up to 90%. 
    2. target variable PH does not seem to have strong correlation with other variables
    
  As a result, we need to remove those pairs with high correlation score.

```{r fig.width=5, fig.height=5}
raw.data %>% 
  select(-c(brand.code)) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  corPlot()
```


## Data Preprocessing

#### Remove Variable

  There are some issue mentioned above where we need to counter them. We need to remove Zero Inflation Variables because these variables does not help much in prediction. Also, the high correlated predictors need to be remove to reduce the instability and bias.

```{r}
# check which column has no prediction effect
zeroInflat <- nearZeroVar(raw.data)

# check highly correlated column
tooHigh <- raw.data %>% 
  select(-c(brand.code)) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  findCorrelation(cutoff = 0.8)

# remove those variables
raw.data <- raw.data[, -tooHigh]
raw.data <- raw.data[, -zeroInflat]
test.data <- test.data[, -tooHigh]
test.data <- test.data[, -zeroInflat]
```


#### Split Data

  Data is not going to split by proportion to perform training and testing task because we have two separate data set which can do the job for us. Therefore, the data is going to split into predictors and target for both.
  
```{r}
trainX <- raw.data %>% select(-c(ph))
trainY <- raw.data %>% select(ph) 
testX <- test.data %>% select(-c(ph))
testY <- test.data %>% select(ph)
```

#### Impute Data

```{r}
impute.result <- trainX %>% 
  mice(m = 1, maxit = 5, seed = 10, printFlag = FALSE)

# save the impute result into trainX, and use the same imputation to impute testX
trainX <- complete(impute.result, 1)
testX <- complete(mice(testX, m = 1, maxit = 5, seed = 10, printFlag = FALSE), 1)
```

#### Create Dummy

  We would like to create dummy variable for `brand.code` because categorical predictors may not be handled directly in model like linear one. So we would like to convert unique categorical predictor into dummy variables
```{r}
trainX <- predict(dummyVars(~., data = trainX), trainX) %>% as.data.frame()
testX <- predict(dummyVars(~., data = testX), testX) %>% as.data.frame()
```
  
#### Transform Data

```{r}
train.trans <- preProcess(trainX, method = c("center", "scale", "YeoJohnson"))
test.trans <- preProcess(testX, method = c("center", "scale", "YeoJohnson"))
trainX <- predict(train.trans, trainX)
testX <- predict(test.trans, testX)
```

## Data Modeling

  We are going to test all the models mentioned in the book,
    
  Linear
  
    Linear Regression
    Partial Least Square
  
  Non Linear
  
    Neural Network
    Support Vector Machine
    K-Nearest Neighboors
    Multivariate Adaptive Regression Splines
  
  Tree 
  
    Recursive Partioning and Regression Trees
    Random Forest
    Gradient Boosts
    Cubist
    
#### Linear   
```{r}
# training control
ctrl <- trainControl(method = "cv")
trainY <- trainY %>% unlist()
```

```{r warning=FALSE}
set.seed(10)

# linear 
lmTune <- train(trainX, trainY,
            method = "lm",
            trControl = ctrl)

# partial least square
plsTune <- train(trainX, trainY,
              method = "pls",
              trControl = ctrl)
```
#### Non Linear
```{r}
set.seed(10)

nnGrid <- expand.grid(.decay = c(0, 0.1, 0.01), .size = c(1:10), .bag = F)
# neural network
nnTune <- train(trainX, trainY,
            method = "avNNet",
            tuneGrid = nnGrid,
            trControl = ctrl,
            linout = T,
            trace = F, 
            maxit = 100,
            maxNWts = 5*(ncol(trainX)+1)+5+1,
            verbose = F
            )
```
```{r}
# SVM
set.seed(10)
svmTune <- train(trainX, trainY,
                 method = "svmRadial", 
                 trControl = ctrl)
```
```{r}
# KNN
set.seed(10)
knnGrid <- expand.grid(.k = c(1:20))
knnTune <- train(trainX, trainY,
                 method = "knn",
                 trControl = ctrl,
                 tuneGrid = knnGrid
                 )
```
```{r}
# MARS
set.seed(10)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:35)
marsTune <- train(trainX, trainY,
                  method = "earth",
                  tuneGrid = marsGrid,
                  trControl = ctrl
                  )
```

#### Tree
```{r}
# recursive partioning and regression tree
set.seed(10)
rpartTune <- train(trainX, trainY,
                   method = "rpart2",
                   trControl = ctrl)
```
```{r}
# random forest
set.seed(10)
rfTune <- train(trainX, trainY,
                method = "rf",
                trControl = ctrl)
```
```{r}
# gradient boost
library(gbm)
gbmGrid <- expand.grid(n.trees = seq(100, 1000, by = 50), interaction.depth = seq(1, 7, by = 1), shrinkage = c(0.01, 0.1), n.minobsinnode = 10)
set.seed(10)
gbmTune <- train(trainX, trainY,
                 method = "gbm",
                 tuneGrid = gbmGrid,
                 trControl = ctrl,
                 verbose = F)
```
```{r}
# cubist
cubistTune <-  train(trainX, trainY,
                     method = "cubist",
                     trControl = ctrl)
```


## Model Evaluation

#### Best Parameters

  We use all model types mentioned and fit the training data. Now the best parameter from each model will be extracted, and use the best model to make prediction.
  
  here in the `lm` model, the best parameter shows that `intercept: 1` which means that intercept is included.
  
  in `nn` model, `bag: 0` means the the parameter is set to False.

```{r}
extract_params <- function(df){
  s <- ""
  for (i in names(df)){
    s = paste0(s, i,": ", round(df[i], 4), " ")
  }
  return(s)
}
```
```{r}
best.param <- 
  cbind("model" = as.character(), "best.params" = as.character()) %>% 
  rbind(
  c("lm", extract_params(lmTune$bestTune)),
  c("pls", extract_params(plsTune$bestTune)),
  c("nn", extract_params(nnTune$bestTune)),
  c("knn", extract_params(knnTune$bestTune)),
  c("svm", extract_params(svmTune$bestTune)),
  c("mars", extract_params(marsTune$bestTune)),
  c("rpart", extract_params(rpartTune$bestTune)),
  c("rf", extract_params(rfTune$bestTune)),
  c("gbm", extract_params(gbmTune$bestTune)),
  c("cubist", extract_params(cubistTune$bestTune))
) %>% as.data.frame()


best.param
```
#### Model Predictions

  we are going to use the best parameters of each individual model to make prediction on the test data.
  
```{r warning=FALSE}
lm.pred <- predict(lmTune, testX)
pls.pred <- predict(plsTune, testX)
nn.pred <- predict(nnTune, testX)
knn.pred <- predict(knnTune, testX)
svm.pred <- predict(svmTune, testX)
mars.pred <- predict(marsTune, testX)
rpart.pred <- predict(rpartTune, testX)
rf.pred <- predict(rfTune, testX)
gbm.pred <- predict(gbmTune, testX)
cubist.pred <- predict(cubistTune, testX)
```

#### Model Performance
```{r}
testY <- testY %>% unlist()
lm.perf <- postResample(lm.pred, testY)
pls.perf <- postResample(pls.pred, testY)
nn.perf <- postResample(nn.pred, testY)
knn.perf <- postResample(knn.pred, testY)
svm.perf <- postResample(svm.pred, testY)
mars.perf <- postResample(mars.pred, testY)
rpart.perf <- postResample(rpart.pred, testY)
rf.perf <- postResample(rf.pred, testY)
gbm.perf <- postResample(gbm.pred, testY)
cubist.perf <- postResample(cubist.pred, testY)
```
```{r}
model.performance <- cbind("model" = as.character(), "RMSE" = as.numeric(), "Rsquared" = as.numeric(), "MAE" = as.numeric()) %>% 
  rbind(
    c("lm", round(lm.perf,4)),
    c("pls", round(pls.perf,4)),
    c("nn", round(nn.perf,4)),
    c("knn", round(knn.perf,4)),
    c("svm", round(svm.perf,4)),
    c("mars", round(mars.perf,4)),
    c("rpart", round(rpart.perf,4)),
    c("rf", round(rf.perf,4)),
    c("gbm", round(gbm.perf,4)),
    c("cubist", round(cubist.perf,4))
  ) %>% as.data.frame()
model.performance
```
```{r}
# RMSE
model.performance %>% 
  arrange(RMSE) %>% 
  mutate(model = factor(model, levels = model)) %>% 
  ggplot(aes(x = model, y = RMSE))+
  geom_bar(stat = "identity") + 
  theme_classic() +
  ggtitle("Model Performance Based on RMSE")
```
```{r}
# Rsquare
model.performance %>% 
  arrange(desc(Rsquared)) %>% 
  mutate(model = factor(model, levels = model)) %>% 
  ggplot(aes(x = model, y = Rsquared))+
  geom_bar(stat = "identity") + 
  theme_classic() +
  ggtitle("Model Performance Based on Rsquared")
```
## Conclusion

  The model performs the best with the least RMSE and most Rsquared is random forest. So, we are going to use Random Forest to model our data.

